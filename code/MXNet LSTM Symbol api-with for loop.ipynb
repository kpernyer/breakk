{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gilbe\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.utils import PROJECT_DATA_DIR\n",
    "import os\n",
    "import mxnet as mx\n",
    "import mxnet.ndarray as nd\n",
    "from time import time\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from load_preprocess import (load_data,\n",
    "                             get_xy,\n",
    "                             scale_data,\n",
    "                             binarize_y,\n",
    "                             prepare_data)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",  category=DeprecationWarning)\n",
    "from DL_mxnet_symbol import train_dnn, train_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = load_data(file='all_training_400_minisensor_1.csv')\n",
    "test = load_data(file='all_test_400_minisensor.csv')\n",
    "xtrain, ytrain, xtest, ytest = prepare_data(train, test, binary_class=True)\n",
    "xtrain_sc, xtest_sc = scale_data(xtrain, xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((91104,), (22776,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ytrain.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_mx = mx.nd.array(xtrain_sc, dtype=np.float32)\n",
    "ytrain_mx = mx.nd.array(ytrain.reshape(-1, 1))\n",
    "xtest_mx = mx.nd.array(xtest_sc, dtype=np.float32)\n",
    "ytest_mx = mx.nd.array(ytest.reshape(-1, 1))\n",
    "batch_size=2**9\n",
    "\n",
    "train_iter = mx.io.NDArrayIter(\n",
    "    xtrain_mx,\n",
    "    ytrain_mx,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "val_iter = mx.io.NDArrayIter(\n",
    "    xtest_mx,\n",
    "    ytest_mx,\n",
    "    batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Already bound, ignoring bind()\n",
      "WARNING:root:optimizer already initialized, ignoring...\n",
      "INFO:root:Epoch[0] Batch [100]\tSpeed: 131102.41 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[0] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[0] Time cost=0.844\n",
      "INFO:root:Epoch[0] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[1] Batch [100]\tSpeed: 113010.47 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[1] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[1] Time cost=0.844\n",
      "INFO:root:Epoch[1] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[2] Batch [100]\tSpeed: 109252.28 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[2] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[2] Time cost=0.828\n",
      "INFO:root:Epoch[2] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[3] Batch [100]\tSpeed: 113011.48 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[3] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[3] Time cost=0.797\n",
      "INFO:root:Epoch[3] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[4] Batch [100]\tSpeed: 112661.38 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[4] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[4] Time cost=0.814\n",
      "INFO:root:Epoch[4] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[5] Batch [100]\tSpeed: 109255.23 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[5] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[5] Time cost=0.765\n",
      "INFO:root:Epoch[5] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[6] Batch [100]\tSpeed: 121377.02 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[6] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[6] Time cost=0.734\n",
      "INFO:root:Epoch[6] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[7] Batch [100]\tSpeed: 121403.65 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[7] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[7] Time cost=0.734\n",
      "INFO:root:Epoch[7] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[8] Batch [100]\tSpeed: 126060.21 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[8] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[8] Time cost=0.719\n",
      "INFO:root:Epoch[8] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[9] Batch [100]\tSpeed: 131090.88 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[9] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[9] Time cost=0.718\n",
      "INFO:root:Epoch[9] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[10] Batch [100]\tSpeed: 131095.76 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[10] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[10] Time cost=0.703\n",
      "INFO:root:Epoch[10] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[11] Batch [100]\tSpeed: 131102.49 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[11] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[11] Time cost=0.734\n",
      "INFO:root:Epoch[11] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[12] Batch [100]\tSpeed: 126060.29 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[12] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[12] Time cost=0.719\n",
      "INFO:root:Epoch[12] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[13] Batch [100]\tSpeed: 126060.95 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[13] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[13] Time cost=0.734\n",
      "INFO:root:Epoch[13] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[14] Batch [100]\tSpeed: 128707.52 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[14] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[14] Time cost=0.726\n",
      "INFO:root:Epoch[14] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[15] Batch [100]\tSpeed: 120913.47 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[15] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[15] Time cost=0.722\n",
      "INFO:root:Epoch[15] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[16] Batch [100]\tSpeed: 126070.06 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[16] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[16] Time cost=0.734\n",
      "INFO:root:Epoch[16] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[17] Batch [100]\tSpeed: 121502.56 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[17] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[17] Time cost=0.741\n",
      "INFO:root:Epoch[17] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[18] Batch [100]\tSpeed: 126063.47 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[18] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[18] Time cost=0.750\n",
      "INFO:root:Epoch[18] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[19] Batch [100]\tSpeed: 126052.37 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[19] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[19] Time cost=0.734\n",
      "INFO:root:Epoch[19] Validation-accuracy=1.000000\n"
     ]
    }
   ],
   "source": [
    "train_dnn(train_iter, val_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36441600, 3)\n",
      "(9110400, 3)\n",
      "shape of xtrain_lstm_sc: (91104, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "xtrain_lstm = xtrain.values.reshape(-1, 3)\n",
    "xtest_lstm = xtest.values.reshape(-1, 3)\n",
    "scaler = QuantileTransformer(output_distribution='normal')\n",
    "xtrain_lstm_sc = scaler.fit_transform(xtrain_lstm)\n",
    "xtest_lstm_sc = scaler.transform(xtest_lstm)\n",
    "\n",
    "print(xtrain_lstm_sc.shape)\n",
    "print(xtest_lstm_sc.shape)\n",
    "\"\"\" Change time steps from 400 to 20 to test if this is the problem\"\"\"\n",
    "xtrain_lstm_sc = mx.nd.array(xtrain_lstm_sc.reshape(-1, 400, 3))\n",
    "val_lstm_sc = mx.nd.array(xtest_lstm_sc.reshape(-1, 400, 3))\n",
    "print('shape of xtrain_lstm_sc:', xtrain_lstm_sc.shape)\n",
    "\n",
    "train_lstm_iter = mx.io.NDArrayIter(\n",
    "    xtrain_lstm_sc,\n",
    "    ytrain_mx,\n",
    "    batch_size,\n",
    "    shuffle=True,\n",
    "    last_batch_handle='discard')\n",
    "\n",
    "val_lstm_iter = mx.io.NDArrayIter(\n",
    "    val_lstm_sc,\n",
    "    ytest_mx,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    "    last_batch_handle='discard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This line is erroneuous\n",
    "#train_lstm(train_lstm_iter, val_lstm_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stacked_rnn_cells = mx.rnn.SequentialRNNCell()\n",
    "#stacked_rnn_cells.add(mx.rnn.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_fused(timesteps=400, \n",
    "              num_layers=3, \n",
    "              mode='lstm', \n",
    "              num_hidden=20,\n",
    "              dropout=0.4,\n",
    "              num_outputs=1, \n",
    "              batch_size=2**9, \n",
    "              input_dim=3):\n",
    "    \n",
    "    data = mx.sym.Variable('data')\n",
    "    \"\"\" Reshape input \"\"\"\n",
    "    input_shape = (timesteps, batch_size, input_dim)\n",
    "    data = mx.sym.Reshape(data, shape=input_shape)\n",
    "    \n",
    "    \"\"\"num_hidden: number of units in output symbol\"\"\"\n",
    "    for i in range(num_layers):\n",
    "        fused_lstm_cell = mx.rnn.FusedRNNCell(\n",
    "            num_hidden=num_hidden, \n",
    "            dropout=dropout)\n",
    "        \"\"\" Implement many layers with for-loop as it is\n",
    "        more effective when using multiple gpus\"\"\"\n",
    "        outputs, _  = fused_lstm_cell.unroll(\n",
    "            length=timesteps, \n",
    "            inputs=data, \n",
    "            merge_outputs=True)\n",
    "        \"\"\" Reshape output from LSTM\"\"\"\n",
    "    output_shape = (batch_size, timesteps, num_hidden)\n",
    "    outputs = mx.sym.Reshape(outputs, shape=output_shape)\n",
    "    outputs = mx.sym.FullyConnected(\n",
    "        data=outputs, \n",
    "        name='out', \n",
    "        num_hidden=num_outputs)\n",
    "    outputs = mx.sym.LogisticRegressionOutput(\n",
    "        outputs,\n",
    "        name='softmax')\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Already bound, ignoring bind()\n",
      "WARNING:root:optimizer already initialized, ignoring...\n",
      "INFO:root:Epoch[0] Batch [100]\tSpeed: 18947.86 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[0] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[0] Time cost=4.753\n",
      "INFO:root:Epoch[0] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[1] Batch [100]\tSpeed: 19670.24 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[1] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[1] Time cost=4.618\n",
      "INFO:root:Epoch[1] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[2] Batch [100]\tSpeed: 19744.14 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[2] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[2] Time cost=4.615\n",
      "INFO:root:Epoch[2] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[3] Batch [100]\tSpeed: 19504.09 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[3] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[3] Time cost=4.679\n",
      "INFO:root:Epoch[3] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[4] Batch [100]\tSpeed: 19734.23 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[4] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[4] Time cost=4.613\n",
      "INFO:root:Epoch[4] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[5] Batch [100]\tSpeed: 19621.04 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[5] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[5] Time cost=4.652\n",
      "INFO:root:Epoch[5] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[6] Batch [100]\tSpeed: 19515.37 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[6] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[6] Time cost=4.651\n",
      "INFO:root:Epoch[6] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[7] Batch [100]\tSpeed: 19583.89 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[7] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[7] Time cost=4.637\n",
      "INFO:root:Epoch[7] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[8] Batch [100]\tSpeed: 19626.06 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[8] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[8] Time cost=4.640\n",
      "INFO:root:Epoch[8] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[9] Batch [100]\tSpeed: 19578.37 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[9] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[9] Time cost=4.630\n",
      "INFO:root:Epoch[9] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[10] Batch [100]\tSpeed: 19583.47 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[10] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[10] Time cost=4.630\n",
      "INFO:root:Epoch[10] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[11] Batch [100]\tSpeed: 19509.61 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[11] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[11] Time cost=4.639\n",
      "INFO:root:Epoch[11] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[12] Batch [100]\tSpeed: 19611.19 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[12] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[12] Time cost=4.631\n",
      "INFO:root:Epoch[12] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[13] Batch [100]\tSpeed: 19518.86 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[13] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[13] Time cost=4.631\n",
      "INFO:root:Epoch[13] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[14] Batch [100]\tSpeed: 19626.18 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[14] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[14] Time cost=4.624\n",
      "INFO:root:Epoch[14] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[15] Batch [100]\tSpeed: 19553.35 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[15] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[15] Time cost=4.649\n",
      "INFO:root:Epoch[15] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[16] Batch [100]\tSpeed: 19640.85 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[16] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[16] Time cost=4.638\n",
      "INFO:root:Epoch[16] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[17] Batch [100]\tSpeed: 19625.92 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[17] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[17] Time cost=4.655\n",
      "INFO:root:Epoch[17] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[18] Batch [100]\tSpeed: 19567.93 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[18] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[18] Time cost=4.653\n",
      "INFO:root:Epoch[18] Validation-accuracy=1.000000\n",
      "INFO:root:Epoch[19] Batch [100]\tSpeed: 19496.08 samples/sec\taccuracy=1.000000\n",
      "INFO:root:Epoch[19] Train-accuracy=1.000000\n",
      "INFO:root:Epoch[19] Time cost=4.657\n",
      "INFO:root:Epoch[19] Validation-accuracy=1.000000\n"
     ]
    }
   ],
   "source": [
    "net = rnn_fused()\n",
    "\n",
    "mod = mx.mod.Module(net, context=mx.gpu())\n",
    "mod.bind(data_shapes=train_lstm_iter.provide_data, \n",
    "        label_shapes=train_lstm_iter.provide_label)\n",
    "mod.init_params(initializer=mx.init.Xavier())\n",
    "mod.init_optimizer(\n",
    "            optimizer='sgd',\n",
    "optimizer_params=(('learning_rate', 0.01), ))\n",
    "\n",
    "mod.fit(train_data=train_iter,\n",
    "        eval_data=val_iter,\n",
    "        #optimizer='s',\n",
    "        #optimizer_params={'learning_rate': 0.01},\n",
    "        eval_metric='acc',\n",
    "        num_epoch=20,\n",
    "        batch_end_callback = mx.callback.Speedometer(batch_size, 100),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define new model\n",
    "ctx = mx.gpu()\n",
    "def rnn(timesteps=400, \n",
    "        nhid=10, \n",
    "        num_layers=1, \n",
    "        dropout=0.4, \n",
    "        batch_size=2**9, \n",
    "        num_outputs=1, \n",
    "        features=3):\n",
    "    \n",
    "    data = mx.sym.Variable('data')\n",
    "    weight = mx.sym.Variable('encoder_weight', \n",
    "                             init=mx.init.Uniform(0.1))\n",
    "\n",
    "    states = []\n",
    "    state_names = []\n",
    "    outputs = mx.sym.Dropout(data, p=dropout)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        prefix = 'lstm_l%d_' % i\n",
    "        cell = mx.rnn.FusedRNNCell(num_hidden=nhid, \n",
    "                                   prefix=prefix,\n",
    "                                   get_next_state=True, \n",
    "                                   forget_bias=0.0,\n",
    "                                   dropout=dropout)\n",
    "        \n",
    "        state_shape= (1, batch_size, nhid)#(batch_size, timesteps, features)\n",
    "        \n",
    "        begin_cell_state_name = prefix + 'cell'\n",
    "        begin_hidden_state_name = prefix + 'hidden'\n",
    "    \n",
    "        begin_cell_state = mx.sym.var(begin_cell_state_name, \n",
    "                                      shape=state_shape)\n",
    "       \n",
    "        begin_hidden_state = mx.sym.var(begin_hidden_state_name, \n",
    "                                        shape=state_shape)\n",
    "        state_names += [begin_cell_state_name, begin_hidden_state_name]\n",
    "        \n",
    "        outputs, next_states = cell.unroll(timesteps, \n",
    "                                           inputs=outputs, \n",
    "                                           begin_state=[begin_cell_state, \n",
    "                                                        begin_hidden_state],\n",
    "                                           merge_outputs=True, \n",
    "                                           layout='TCN')\n",
    "        \n",
    "        outputs = mx.sym.Dropout(outputs, p=0.4)\n",
    "        states += next_states\n",
    "    outputs = mx.sym.Reshape(outputs, shape=(-1, nhid))\n",
    "    \n",
    "    outputs = mx.sym.FullyConnected(data=outputs, name='out', \n",
    "                                    num_hidden=num_outputs)\n",
    "    \n",
    "    outputs = mx.sym.LogisticRegressionOutput(outputs,\n",
    "                                              name='softmax')\n",
    "\n",
    "    \n",
    "    return outputs, [mx.sym.stop_gradient(s) for s in states], state_names\n",
    "    \n",
    "    \n",
    "\n",
    "def softmax_ce_loss(pred):\n",
    "    # softmax cross-entropy loss\n",
    "    label = mx.sym.Variable('label')\n",
    "    label = mx.sym.Reshape(label, shape=(-1,))\n",
    "    logits = mx.sym.log_softmax(pred, axis=-1)\n",
    "    loss = -mx.sym.pick(logits, label, axis=-1, keepdims=True)\n",
    "    loss = mx.sym.mean(loss, axis=0, exclude=True)\n",
    "    return mx.sym.make_loss(loss, name='nll')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from module import CustomStatefulModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#pred, states, state_names = rnn()\n",
    "#loss = softmax_ce_loss(pred)\n",
    "#module = CustomStatefulModule(loss, \n",
    "#                              states, \n",
    "#                              state_names=state_names, \n",
    "#                              context=ctx)\n",
    "#module.bind(data_shapes=train_lstm_iter.provide_data, \n",
    "#            label_shapes=train_lstm_iter.provide_label)\n",
    "#module.init_params(initializer=mx.init.Xavier())\n",
    "#optimizer = mx.optimizer.create('sgd', \n",
    "#                                learning_rate=0.01, \n",
    "#                                rescale_grad=1.0/batch_size)\n",
    "#module.init_optimizer(optimizer=optimizer)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
